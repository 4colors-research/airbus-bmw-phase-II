'''
This file is part of Garona. 
(c) 4colors Research Ltd, 2024

This file contains the implementation of the reinforcement learning routine.
'''


import lightgbm as lgb
import numpy as np
import pandas as pd
import math
from itertools import combinations
from copy import copy, deepcopy
from sklearn.linear_model import LogisticRegression

class PredictionModel:

    def __init__(self, features, labels, model_nr, constraint_map):
        """
        :param features: list of hashmaps containing features collected from iterations of the LP model
        :param labels: list with the number of non-integer values assigned in different iterations of the LP model
        :param model_nr: which dataset generated by iterations of RL to use to train the model,  if <0 use a pretrained model
        :param constrain_map: a hast map for variables to constraints containing the variables 
        """
        assert len(features) == len(labels)
        self.constraint_map = constraint_map
        self.type_map = {'T': 1, 't': 2, 'c': 3} # hard-coded as we assume we use variables of these types for rounding
        if len(features) > 0: # use for generating new data and training the model
            self.X = self.generate_X(features) 
            self.y = self.generate_binary_labels(labels)
            assert self.X.shape[0] == len(self.y)
            self.model = self.train()
        else:    # use for inference
            if model_nr<0:      # load model from file
                print('Loading precomputed LightGBM model')
                self.model = lgb.Booster(model_file='./../data/models/lgb_model.txt') # hardcoded for now
                df = pd.read_csv('./../data/models/df_columns.csv')
                self.all_data_vars = {col: i for i, col in enumerate(df)}
                self.X = self.transform_X(df)
            else:    # load dataset from file and train the model
                self.model = self.train_from_file(model_nr)

    def add_example(self, feature_vector, label):
        self.X.append(feature_vector)
        self.y.append(label)
    
    def train_logreg(self):
        """
            Logistic regression model, just for comparison of performance
        """
        model = LogisticRegression()
        model.fit(self.transform_X(self.X), self.y)
        return model

    def train(self):
        """
            Traing a LightGBM model. The parameters below have been found using grid-search
        """
        params = {
            "boosting_type": "gbdt",
            "objective": "binary",
            "metric": "accuracy",
            "num_leaves": 21,
            "learning_rate": 0.025,
            "feature_fraction": 0.7,
            'bagging_freq': 1,
            "verbose": 0,
            "min_data": 5
        }
        lgb_train = lgb.Dataset(self.transform_X(self.X), self.y)
        return lgb.train(params, lgb_train, num_boost_round=10)
    
    def train_from_file(self, model_nr):
        """
            Load the dataset computed using RL and train a GBT model on it
        """
        print("Training model")
        df = pd.read_csv(f"./../data/training_data/dense_X_epoch_{model_nr}.csv") # good result with 374
        df = self.summarize_X(df)
        self.all_data_vars = {col: i for i, col in enumerate(df.columns)}
        y = df['y']
        X = self.transform_X(df) #df[cols]
        print('Training X shape', X.shape)
        params = {
            "boosting_type": "gbdt",
            "objective": "binary",
            "metric": "accuracy",
            "num_leaves": 21,
            "learning_rate": 0.025,
            "feature_fraction": 0.7,
            'bagging_freq': 1,
            "verbose": 0,
            "min_data": 5
        }
        lgb_train = lgb.Dataset(X, y)
        self.X, self.y = X, y
        return lgb.train(params, lgb_train, num_boost_round=30)

    def predict(self, df):
        
        assert df.shape[1] == self.X.shape[1]
        for col_df, col_X in zip(df.columns, self.X.columns):
            assert col_df == col_X
        return self.model.predict(self.transform_X(df))
    
    def transform_X(self, X):
        """
            Feature selection from the original data matrix.
            Analysis was performed in a separate notebook.
        """
        cols = [col for col in X.columns if 'selected_' in col or 'iter' in col or 'fal_' in col \
                or 'constraint_' in col or 'var_' in col or 'nonint_' in col]
        return X[cols]
    
    def generate_binary_labels(self, labels): 
        """
            Given a list with the number of variables with non-integer values in separate iterations, generate binary labels.  
            A label is a tuple of (non_int, min_non_int) for each epoch 
        """
        y = []
        label_prev = labels[0]
        #print('labels', labels)
        for label in labels:
            # print(f'prev: {label_prev}, now: {label}, y: {max(0, 1-int(label[0]/label_prev[0]))}')
            y.append(max(0, 1-int(label[0]/label_prev[0]))) # does the score improve in the next iteration
            label_prev = label
        return y

    def generate_X(self, features):
        """
            Given a list of feature hash maps, convert them into a pandas dataframe.
        """
        self.all_data_vars = {}
        self.all_fals = {}
        for feature in features:
            for var in feature['non_int_map']:
                self.all_data_vars.setdefault(var, len(self.all_data_vars))
            for var in feature['selected_vars']:
                self.all_data_vars.setdefault(var, len(self.all_data_vars))
            for fal in feature['FAL_distribution']:
                self.all_fals.setdefault(fal, len(self.all_fals))
        rows = []
        l1, l2= len(self.all_data_vars), len(self.all_fals) 
        for feature in features:
            row = [0]*l1 + [-1]*(l1+l2) # by default values are set to 0
            for var, idx in self.all_data_vars.items():
                if var in feature['nnz_map']: # value diff from 0
                    row[idx] = feature['nnz_map'][var]
                if var in feature['non_int_map']: # value diff from 0 and not integer
                    row[idx] = feature['non_int_map'][var]
                if var in feature['selected_vars']:
                    row[l1 + idx] = feature['selected_vars'][var]
            for fal, value in feature['FAL_distribution'].items():
                row[2*l1+self.all_fals[fal]] = value
            combos = combinations(feature['selected_vars'], 2)
            for comb in combos:
                intrs = self.constraint_map[comb[0]].intersection(self.constraint_map[comb[1]])
                #print('comb', comb, len(intrs), self.constraint_map[comb[0]], self.constraint_map[comb[1]])
                row.append(len(intrs))
            for var in feature['selected_vars']:
                #self.type_map.setdefault(var[0], len(self.type_map))
                row.append(self.type_map[var[0]])
            row.append(feature['iter'])
            rows.append(row)
        
        n = len(features[0]['selected_vars'])
        rev_fals = {idx:name for name, idx in self.all_fals.items()}

        # the order is the same as in the for-loops above
        self.columns = list(self.all_data_vars.keys()) + ['selected_' + name for name in list(self.all_data_vars.keys())] + \
            ['fal_' + rev_fals[idx][:-4] for idx in sorted(rev_fals.keys())] + ['constraint_comb_' + str(i) for i in range((n*(n-1))//2)] \
              +  ['var_type_'+str(i) for i in range(len(features[0]['selected_vars']))] + ['iter']
        assert len(self.columns) == len(rows[0])
        df = pd.DataFrame(rows)
        df.columns = self.columns
        return df
    

    def simulate_X_inference(self, feature_map, solution_dict, filtered_non_integral_vars, n_vars_to_round, seed, random_samples):
        """
            Given features as a hash map, convert them into a pandas dataframe
            and generate a <random_samples> number of tuples that will be scored by the classification model.
            The function is used for inference and assumes the template of the data is already computed.
        """
        var_dict = {col: [np.nan] for col in self.X.columns}
        for var, val in feature_map['nnz_map'].items():
            if var in var_dict:
                var_dict[var][0] = val
        for var, val in feature_map['non_int_map'].items():
            if 'selected_' + var in var_dict:
                var_dict['selected_' + var][0] = val
        for var, val in feature_map['FAL_distribution'].items():
            var = var.split()[0]
            if 'fal_' + var in var_dict:
                var_dict['fal_' + var][0] = val
        var_dict['iter'][0] = feature_map['iter']
        df = pd.DataFrame(var_dict)
        row = dict(df.iloc[0])
        overfiltered_non_int_vars = [v for v in filtered_non_integral_vars if v in self.all_data_vars]

        # generate random tuples for scoring by the model
        np.random.seed(42)
        vars_tuples = np.random.choice(overfiltered_non_int_vars, size=(random_samples, n_vars_to_round))
        rows = []
        missed = 0
        valid_tuples = []
        for tpl in vars_tuples:
            if len(set(tpl)) < n_vars_to_round:
                missed += 1
                continue
            row_cp = copy(row)
            for var in tpl:
                if var in row_cp:
                    row_cp[var] = math.floor(solution_dict[var])
            combos = combinations(list(tpl), 2)
            for i, comb in enumerate(combos):
                intrs = self.constraint_map[comb[0]].intersection(self.constraint_map[comb[1]])
                row_cp['constraint_comb_' + str(i)] = len(intrs)
            for i, var in enumerate(tpl):
                row_cp['var_type_'+str(i)] = self.type_map[var[0]]#, -1)
            rows.append(row_cp)
            valid_tuples.append(tpl)
        if len(rows) == 0:
            return None, []
            
        df = pd.DataFrame(rows)
        df = self.summarize_X(df) # generate additional features
        return df[self.X.columns], valid_tuples

    def simulate_X(self, feature_map, solution_dict, filtered_non_integral_vars, n_vars_to_round, seed, random_samples=50):
        """
            Given features as a hash map, convert them into a pandas dataframe
            and generate a <random_samples> number of tuples that will be scored by the classification model.
            The function is used for training when the dataset is generated.
        """
        l1, l2 = len(self.all_data_vars), len(self.all_fals)
        row = [0]*l1 + [-1]*(l1+l2)
        for var, idx in self.all_data_vars.items():
            if var in feature_map['nnz_map']: # value different from 0
                row[idx] = feature_map['nnz_map'][var]
            if var in feature_map['non_int_map']: # value different from 0 and not integer
                row[idx] = feature_map['non_int_map'][var]
        for fal, value in feature_map['FAL_distribution'].items():
            row[2*l1 + self.all_fals[fal]] = value

        overfiltered_non_int_vars = [v for v in filtered_non_integral_vars if v in self.all_data_vars]
        print('after filtering', len(filtered_non_integral_vars), len(overfiltered_non_int_vars))
        np.random.seed(seed)
        vars_tuples = np.random.choice(overfiltered_non_int_vars, size=(random_samples, n_vars_to_round))
        rows = []
        missed = 0
        valid_tuples = []
        for tpl in vars_tuples:
            if len(set(tpl)) < n_vars_to_round:
                missed += 1
                continue
            row_cp = copy(row)
            for var in tpl:
                row_cp[l1+self.all_data_vars[var]] = math.floor(solution_dict[var])
            combos = combinations(list(tpl), 2)
            for comb in combos:
                intrs = self.constraint_map[comb[0]].intersection(self.constraint_map[comb[1]])
                row_cp.append(len(intrs))
            for var in tpl:
                row_cp.append(self.type_map.get(var[0], -1))
            row_cp.append(feature_map['iter'])
            rows.append(row_cp)
            valid_tuples.append(tpl)
        if len(rows) == 0:
            raise Exception("Dataset is empty")
        df = pd.DataFrame(rows)
        assert df.shape[0] + missed == random_samples
        assert df.shape[0] == len(valid_tuples)
        assert df.shape[1] == len(self.columns)
        df.columns = self.columns
        return df, valid_tuples
    

    def cnt_nonint(self, l):
        return sum([int(abs(li-int(li))>1e-5) for li in l])
    
    def summarize_X(self, df):
        cols_vars = [col for col in df.columns if 'selected' not in col and 'iter' not in col \
                and 'fal_' not in col and 'constraint_' not in col and 'var_' not in col and col != 'y']
        df_T = df[[col for col in cols_vars if col[0]=='T']]
        df_t = df[[col for col in cols_vars if col[0]=='t']]
        df_c = df[[col for col in cols_vars if col[0]=='c']]
        
        df['nonint_T'] = df_T.apply(lambda row: self.cnt_nonint(row), axis=1)
        df['nonint_t'] = df_t.apply(lambda row: self.cnt_nonint(row), axis=1)
        df['nonint_c'] = df_c.apply(lambda row: self.cnt_nonint(row), axis=1)

        return df